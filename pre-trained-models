#==========================================================================================================
#Base Set Model
#==========================================================================================================

# import os
# import numpy as np
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.image import load_img, img_to_array
# from keras.models import Sequential
# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

#def load_dataset(dataset_dir, image_size=(224, 224)):
#     images = []
#     labels = []

#     for filename in os.listdir(dataset_dir):
#         if filename.endswith('.jpg'):
#             image = load_img(os.path.join(dataset_dir, filename), target_size=image_size)
#             image = img_to_array(image) / 255.0
#             images.append(image)
#             labels.append(filename.split('_')[1])

#     return np.array(images), np.array(labels)

# def build_model(input_shape, num_classes):
#     model = Sequential([
#         Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
#         MaxPooling2D((2, 2)),
#         Conv2D(64, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Conv2D(128, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     return model

# dataset_dir = '/home/brandon/Downloads/archive/tf'
# images, labels = load_dataset(dataset_dir)

# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}
# y_train = np.array([label_mapping[label] for label in y_train])
# y_val = np.array([label_mapping[label] for label in y_val])

# input_shape = X_train.shape[1:]
# num_classes = len(np.unique(labels))
# model = build_model(input_shape, num_classes)

# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))


# print(history.history.keys())  
# import matplotlib.pyplot as plt

# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('Model loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# val_loss, val_acc = model.evaluate(X_val, y_val)
# print(f'Validation accuracy: {val_acc}')

#==========================================================================================================
#VGG16 Pre-Trained Model
#==========================================================================================================

import os
import numpy as np
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import load_img, img_to_array
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout
from keras.optimizers import Adam

def load_dataset(dataset_dir, image_size=(224, 224)):
    images = []
    labels = []

    for filename in os.listdir(dataset_dir):
        if filename.endswith('.jpg'):
            image = load_img(os.path.join(dataset_dir, filename), target_size=image_size)
            image = img_to_array(image) / 255.0
            images.append(image)
            labels.append(filename.split('_')[1])

    return np.array(images), np.array(labels)

dataset_dir = '/home/brandon/Downloads/archive/tf'
images, labels = load_dataset(dataset_dir)

X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}
y_train = np.array([label_mapping[label] for label in y_train])
y_val = np.array([label_mapping[label] for label in y_val])

# Load pre-trained VGG16 model
vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze VGG16 layers
for layer in vgg16.layers:
    layer.trainable = False

# Create model
model = Sequential([
    vgg16,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate model
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f'Validation accuracy: {val_acc}') 

#==========================================================================================================
#DenseNet201 Pre-Trained Model
#==========================================================================================================

# import os
# import numpy as np
# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.image import load_img, img_to_array
# from keras.models import Sequential
# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# def load_dataset(dataset_dir, image_size=(224, 224)):
#     images = []
#     labels = []

#     for filename in os.listdir(dataset_dir):
#         if filename.endswith('.jpg'):
#             image = load_img(os.path.join(dataset_dir, filename), target_size=image_size)
#             image = img_to_array(image) / 255.0
#             images.append(image)
#             labels.append(filename.split('_')[1])

#     return np.array(images), np.array(labels)

# def build_model(input_shape, num_classes):
#     model = Sequential([
#         Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
#         MaxPooling2D((2, 2)),
#         Conv2D(64, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Conv2D(128, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dropout(0.5),
#         Dense(num_classes, activation='softmax')
#     ])
#     return model

# # Load dataset
# dataset_dir = '/home/brandon/Downloads/archive/tf'
# images, labels = load_dataset(dataset_dir)

# # Encode labels
# label_encoder = LabelEncoder()
# labels = label_encoder.fit_transform(labels)

# # Split dataset into training and validation sets
# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# input_shape = X_train.shape[1:]
# num_classes = len(np.unique(labels))
# model = build_model(input_shape, num_classes)

# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# print(history.history.keys())  

# import matplotlib.pyplot as plt

# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('Model loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# val_loss, val_acc = model.evaluate(X_val, y_val)
# print(f'Validation accuracy: {val_acc}')

#==========================================================================================================
#ResNet50 (18 Layers) Pre-Trained Model
#==========================================================================================================
# import os
# import numpy as np
# from sklearn.preprocessing import LabelEncoder
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.image import load_img, img_to_array
# from keras.models import Sequential
# from keras.layers import Dense, GlobalAveragePooling2D, Dropout
# from keras.applications import ResNet50
# from keras.callbacks import ModelCheckpoint

# def load_dataset(dataset_dir, image_size=(224, 224)):
#     images = []
#     labels = []

#     for filename in os.listdir(dataset_dir):
#         if filename.endswith('.jpg'):
#             image = load_img(os.path.join(dataset_dir, filename), target_size=image_size)
#             image = img_to_array(image) / 255.0
#             images.append(image)
#             labels.append(filename.split('_')[1])

#     return np.array(images), np.array(labels)

# # Load dataset
# dataset_dir = '/home/brandon/Downloads/archive/tf'
# images, labels = load_dataset(dataset_dir)

# # Encode labels
# label_encoder = LabelEncoder()
# labels = label_encoder.fit_transform(labels)

# # Split dataset into training and validation sets
# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)

# # Load ResNet50 pre-trained model without the top layer
# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# # Add custom layers on top of the pre-trained model
# model = Sequential([
#     base_model,
#     GlobalAveragePooling2D(),
#     Dense(256, activation='relu'),
#     Dropout(0.5),
#     Dense(len(np.unique(labels)), activation='softmax')
# ])

# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Define a checkpoint callback to save the best model during training
# checkpoint = ModelCheckpoint("best_model.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[checkpoint])

# print(history.history.keys())  

# import matplotlib.pyplot as plt

# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('Model loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper left')
# plt.show()

# val_loss, val_acc = model.evaluate(X_val, y_val)
# print(f'Validation accuracy: {val_acc}')
